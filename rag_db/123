# Проект: RAG база на Elasticsearch (папка модуля)

Ниже приведена структура проекта и все файлы, которые я сгенерировал. Откройте документ и скопируйте нужные файлы в папку проекта.

---

## Структура

```
rag_elastic/
├── docker-compose.yml
├── README.md
├── requirements.txt
├── rag_db/
│   ├── __init__.py
│   ├── es_client.py
│   ├── utils.py
│   ├── indexer.py
│   ├── searcher.py
│   └── example.py
```

---

## File: docker-compose.yml

```yaml
version: '3.8'
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.8.1
    container_name: es-rag
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - esdata:/usr/share/elasticsearch/data
    ports:
      - 9200:9200
volumes:
  esdata:
```

> Примечание: Эта конфигурация запускает Elasticsearch в режиме single-node с отключённой безопасностью — удобно для локальной разработки. Для продакшена необходимо включить безопасность и настроить пароли/сертификаты.

---

## File: requirements.txt

```
elasticsearch>=8.5.0
sentence-transformers>=2.2.2
numpy>=1.22
tqdm
python-dateutil
```

---

## File: README.md

```md
# RAG Elasticsearch module

Запуск:

1. `docker-compose up -d`
2. Установить зависимости: `pip install -r requirements.txt`
3. Прописать и положить свои документы в example.py или используйте свой парсер для Telegram
4. Запустить `python -m rag_db.example`

Функции модуля:
- создание индекса с dense_vector
- индексирование документов батчами
- семантический (векторный) поиск
- SQL-like запросы через Elasticsearch SQL endpoint
- Ctrl+F (фразовый / полнотекстовый) поиск

```

---

## File: rag_db/**init**.py

```python
# rag_db package
__all__ = ["es_client", "utils", "indexer", "searcher"]
```

---

## File: rag_db/es_client.py

```python
from elasticsearch import Elasticsearch
import os

ES_HOST = os.getenv('ES_HOST', 'http://localhost:9200')

def get_client():
    # Простая обёртка — в продакшене добавьте retry/backoff и аутентификацию
    return Elasticsearch(ES_HOST)
```

---

## File: rag_db/utils.py

```python
from sentence_transformers import SentenceTransformer
import numpy as np

# Выберите модель — для локальной разработки подойдёт 'all-MiniLM-L6-v2'.
# При желании можно заменить на более мощную модель.
EMBED_MODEL_NAME = 'all-MiniLM-L6-v2'

class Embedder:
    def __init__(self, model_name=EMBED_MODEL_NAME):
        self.model = SentenceTransformer(model_name)

    def embed(self, texts):
        """Возвращает список векторов float32 для списка texts"""
        embs = self.model.encode(texts, show_progress_bar=False, convert_to_numpy=True)
        # ensure float32
        return embs.astype(np.float32)
```

---

## File: rag_db/indexer.py

```python
import time
from elasticsearch import exceptions as es_ex
from .es_client import get_client
from .utils import Embedder

DEFAULT_INDEX = 'news_rag'
EMBED_DIM = 384  # для all-MiniLM-L6-v2

INDEX_MAPPING = {
    "mappings": {
        "properties": {
            "title": {"type": "text"},
            "channel": {"type": "keyword"},
            "published_at": {"type": "date"},
            "raw_text": {"type": "text", "analyzer": "standard"},
            "clean_text": {"type": "text", "analyzer": "standard"},
            "embedding": {"type": "dense_vector", "dims": EMBED_DIM},
            "meta": {"type": "object", "enabled": False}
        }
    }
}


class Indexer:
    def __init__(self, index_name=DEFAULT_INDEX):
        self.client = get_client()
        self.index = index_name
        self.embedder = Embedder()

    def create_index(self, force: bool = False):
        if self.client.indices.exists(index=self.index):
            if force:
                self.client.indices.delete(index=self.index)
            else:
                return
        self.client.indices.create(index=self.index, body=INDEX_MAPPING)

    def index_documents(self, docs, batch_size=64):
        # docs: list of dicts with fields: id (optional), title, channel, published_at, raw_text, clean_text, meta
        from elasticsearch import helpers
        actions = []
        texts = [d.get('clean_text') or d.get('raw_text') for d in docs]
        embeddings = self.embedder.embed(texts)
        for i, doc in enumerate(docs):
            action = {
                "_op_type": "index",
                "_index": self.index,
            }
            if 'id' in doc:
                action['_id'] = doc['id']
            # copy fields
            body = {
                'title': doc.get('title'),
                'channel': doc.get('channel'),
                'published_at': doc.get('published_at'),
                'raw_text': doc.get('raw_text'),
                'clean_text': doc.get('clean_text'),
                'embedding': embeddings[i].tolist(),
                'meta': doc.get('meta', {})
            }
            action['_source'] = body
            actions.append(action)
        helpers.bulk(self.client, actions)

    def count(self):
        return self.client.count(index=self.index)['count']
```

---

## File: rag_db/searcher.py

```python
from .es_client import get_client
from .utils import Embedder
import numpy as np

DEFAULT_INDEX = 'news_rag'

class Searcher:
    def __init__(self, index_name=DEFAULT_INDEX):
        self.client = get_client()
        self.index = index_name
        self.embedder = Embedder()

    def semantic_search(self, query: str, k: int = 5, field: str = 'clean_text'):
        # Получаем эмбеддинг запроса
        q_emb = self.embedder.embed([query])[0].tolist()
        # Используем script_score с cosine similarity: cosineSimilarity(params.query_vector, 'embedding')
        script_query = {
            "script_score": {
                "query": {"match_all": {}},
                "script": {
                    "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                    "params": {"query_vector": q_emb}
                }
            }
        }
        resp = self.client.search(index=self.index, body={
            "size": k,
            "query": script_query,
            "_source": ["title","channel","published_at","clean_text","raw_text","meta"]
        })
        hits = resp['hits']['hits']
        results = []
        for h in hits:
            results.append({
                'id': h['_id'],
                'score': h['_score'],
                'source': h['_source']
            })
        return results

    def ctrl_f_search(self, phrase: str, k: int = 20, field: str = 'raw_text'):
        # быстрый фразовый поиск — match_phrase. Для простой Ctrl+F можно заменить на wildcard/regexp
        body = {
            "size": k,
            "query": {
                "match_phrase": {
                    field: {
                        "query": phrase
                    }
                }
            },
            "_source": ["title","channel","published_at","clean_text","raw_text","meta"]
        }
        resp = self.client.search(index=self.index, body=body)
        return [{ 'id': h['_id'], 'score': h['_score'], 'source': h['_source'] } for h in resp['hits']['hits']]

    def sql_search(self, sql_query: str):
        # Elasticsearch SQL API — вернёт табличный результат
        # Пример: "SELECT title, channel FROM news_rag WHERE channel = 'my_channel' ORDER BY published_at DESC LIMIT 10"
        res = self.client.sql.query(body={"query": sql_query})
        cols = [c['name'] for c in res['columns']]
        rows = res['rows']
        results = [dict(zip(cols, r)) for r in rows]
        return results
```

---

## File: rag_db/example.py

```python
"""
Пример использования модуля.

Сначала запустите ES: docker-compose up -d
Потом установите зависимости.
"""
from rag_db.indexer import Indexer
from rag_db.searcher import Searcher
import datetime

SAMPLE_DOCS = [
    {
        'id': '1',
        'title': 'Пример новости 1',
        'channel': 'channel_a',
        'published_at': datetime.datetime.utcnow().isoformat(),
        'raw_text': 'Сегодня произошло событие: кто-то сказал что-то субъективное.',
        'clean_text': 'Сегодня произошло событие.'
    },
    {
        'id': '2',
        'title': 'Пример новости 2',
        'channel': 'channel_b',
        'published_at': datetime.datetime.utcnow().isoformat(),
        'raw_text': 'Ещё одна новость с оценочным суждением: это плохо.',
        'clean_text': 'Ещё одна нейтральная новость.'
    }
]


def run_example():
    idx = Indexer()
    print('Создаю индекс (force=True) ...')
    idx.create_index(force=True)
    print('Индексирую примеры ...')
    idx.index_documents(SAMPLE_DOCS)
    print('Всего документов в индексе:', idx.count())

    s = Searcher()
    print('\nСемантический поиск по запросу: "событие"')
    res = s.semantic_search('событие', k=3)
    for r in res:
        print(r['id'], r['score'], r['source']['title'])

    print('\nCtrl+F (match phrase) для "оценочным"')
    res2 = s.ctrl_f_search('оценочным')
    print(res2)

    print('\nSQL search пример:')
    print(s.sql_search("SELECT id, title, channel FROM news_rag ORDER BY published_at DESC LIMIT 5"))

if __name__ == '__main__':
    run_example()
```

---

# Конец

Откройте документ и скопируйте/скачайте файлы. Если хотите — могу сразу адаптировать этот код под OpenSearch или PostgreSQL + pgvector, добавить async-версии, автотесты, CI или Dockerfile для приложения. Скажите, что нужно улучшить дальше.
